{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic Sentiment Analysis.ipynb",
      "provenance": [],
      "mount_file_id": "11930jR-6afqwx3Hbn2TGRGN0p7Asixr0",
      "authorship_tag": "ABX9TyO1EWjfWQJMvN/IphYmKIgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysfesr/YT-COMMENTS-ANALYSIS/blob/master/Arabic_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBpUBp1xkfjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100793cc-6b88-4487-ad77-cf1863c34b23"
      },
      "source": [
        "import pandas as pd\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rzeO0bBQgtQ6",
        "outputId": "cccb2a90-c8cf-40f7-b8da-fea0eb7eefe3"
      },
      "source": [
        "data = pd.read_csv('drive/MyDrive/colab notebook/Arabe Sentiment Analysis/datasets/arabic_sentiment.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>#الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>الدودو جايه تكمل علي 💔</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  label                                               text\n",
              "0           0      0  اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...\n",
              "1           1      0  توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...\n",
              "2           2      0  #الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...\n",
              "3           3      0  نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...\n",
              "4           4      0                             الدودو جايه تكمل علي 💔"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1k2llo9mShG"
      },
      "source": [
        "stop_words = set(stopwords.words('arabic'))\r\n",
        "\r\n",
        "def remove_diacritics(text):\r\n",
        "    arabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\r\n",
        "                             َ    | # Fatha\r\n",
        "                             ً    | # Tanwin Fath\r\n",
        "                             ُ    | # Damma\r\n",
        "                             ٌ    | # Tanwin Damm\r\n",
        "                             ِ    | # Kasra\r\n",
        "                             ٍ    | # Tanwin Kasr\r\n",
        "                             ْ    | # Sukun\r\n",
        "                             ـ     # Tatwil/Kashida\r\n",
        "                         \"\"\", re.VERBOSE)\r\n",
        "    text = re.sub(arabic_diacritics, '', str(text))\r\n",
        "    return text\r\n",
        "\r\n",
        "def remove_emoji(text):\r\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags = re.UNICODE)\r\n",
        "    return regrex_pattern.sub(r'',text)\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    text = \"\".join([word for word in text if word not in string.punctuation])\r\n",
        "    text = remove_emoji(text)\r\n",
        "    text = remove_diacritics(text)\r\n",
        "    tokens = word_tokenize(text)\r\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\r\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "epqSnzrAohuH",
        "outputId": "c51e20ad-8045-45e3-b2c5-0c488f4f5d5e"
      },
      "source": [
        "data = data.drop('Unnamed: 0', axis=1)\r\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\r\n",
        "data.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23431</th>\n",
              "      <td>1</td>\n",
              "      <td>سحب على مبلغ مالي 💰 لمتابعي #كشكول 👍🏻 المطلوب:...</td>\n",
              "      <td>سحب مبلغ مالي لمتابعي كشكول المطلوب شي رتويت ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24029</th>\n",
              "      <td>1</td>\n",
              "      <td>افف كيوت مرا</td>\n",
              "      <td>افف كيوت مرا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35487</th>\n",
              "      <td>1</td>\n",
              "      <td>اهبل مايعرفني باقي 😏</td>\n",
              "      <td>اهبل مايعرفني باقي</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20589</th>\n",
              "      <td>0</td>\n",
              "      <td>اصطبحي وقولي يا صبح 😏</td>\n",
              "      <td>اصطبحي وقولي صبح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26301</th>\n",
              "      <td>1</td>\n",
              "      <td>لاعب #الهلال كنو في مباراة الامس: التمريرات ✅ ...</td>\n",
              "      <td>لاعب الهلال كنو مباراة الامس التمريرات ✅ دقة ا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ...                                       cleaned_text\n",
              "23431      1  ...  سحب مبلغ مالي لمتابعي كشكول المطلوب شي رتويت ا...\n",
              "24029      1  ...                                       افف كيوت مرا\n",
              "35487      1  ...                                 اهبل مايعرفني باقي\n",
              "20589      0  ...                                   اصطبحي وقولي صبح\n",
              "26301      1  ...  لاعب الهلال كنو مباراة الامس التمريرات ✅ دقة ا...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB33APlFojf5"
      },
      "source": [
        "# split the data to tain and test data\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'], test_size=0.3, stratify=data['label'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G1vzFw4qFn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b33f74-143d-4bd4-a6cb-e82211d2c262"
      },
      "source": [
        "steps = [('vectorizer', TfidfVectorizer(min_df=0.0001, max_df=0.95, analyzer='word', lowercase=False)), ('cls', RandomForestClassifier())]\r\n",
        "pipeline = Pipeline(steps) # define the pipeline object.\r\n",
        "parameteres = {'vectorizer__max_features':[1000,3000,7000,10000]}\r\n",
        "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\r\n",
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vectorizer',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=False,\n",
              "                                                        max_df=0.95,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=0.0001,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=No...\n",
              "                                                               min_samples_leaf=1,\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=None,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'vectorizer__max_features': [1000, 3000, 7000, 10000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bQdUd3uqxug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0859c4fa-1357-45b7-8592-99e3a5da4325"
      },
      "source": [
        "print(\"best estimator: \",grid.best_estimator_)\r\n",
        "print(\"best score: \", grid.best_score_)\r\n",
        "print(\"best params: \", grid.best_params_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best estimator:  Pipeline(memory=None,\n",
            "         steps=[('vectorizer',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.float64'>,\n",
            "                                 encoding='utf-8', input='content',\n",
            "                                 lowercase=False, max_df=0.95,\n",
            "                                 max_features=10000, min_df=0.0001,\n",
            "                                 ngram_range=(1, 1), norm='l2',\n",
            "                                 preprocessor=None, smooth_idf=True,\n",
            "                                 stop_words=None, strip_accents=None,\n",
            "                                 sublinear_tf=False,\n",
            "                                 to...\n",
            "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                                        class_weight=None, criterion='gini',\n",
            "                                        max_depth=None, max_features='auto',\n",
            "                                        max_leaf_nodes=None, max_samples=None,\n",
            "                                        min_impurity_decrease=0.0,\n",
            "                                        min_impurity_split=None,\n",
            "                                        min_samples_leaf=1, min_samples_split=2,\n",
            "                                        min_weight_fraction_leaf=0.0,\n",
            "                                        n_estimators=100, n_jobs=None,\n",
            "                                        oob_score=False, random_state=None,\n",
            "                                        verbose=0, warm_start=False))],\n",
            "         verbose=False)\n",
            "best score:  0.7754553038264804\n",
            "best params:  {'vectorizer__max_features': 10000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-N9ZBLUq36n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6237867-f85f-4aaf-a01e-89a0a339d87a"
      },
      "source": [
        "y_pred = grid.predict(X_test)\r\n",
        "precision = precision_score(y_test, y_pred)\r\n",
        "recall = recall_score(y_test, y_pred)\r\n",
        "accuracy = accuracy_score(y_test, y_pred)\r\n",
        "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.799 / Recall: 0.749 / Accuracy: 0.779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGhzP_0qAMUK",
        "outputId": "66984add-b7c1-45d3-c94c-23aea2869984"
      },
      "source": [
        "from sklearn.externals import joblib\r\n",
        "\r\n",
        "model_save_name = 'sentiment_classifier.pkl'\r\n",
        "path = F\"drive/MyDrive/colab notebook/Arabe Sentiment Analysis/{model_save_name}\"\r\n",
        "\r\n",
        "joblib.dump(grid.best_estimator_, path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/colab notebook/Arabe Sentiment Analysis/sentiment_classifier.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}