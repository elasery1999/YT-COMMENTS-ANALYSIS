{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic Sentiment Analysis.ipynb",
      "provenance": [],
      "mount_file_id": "11930jR-6afqwx3Hbn2TGRGN0p7Asixr0",
      "authorship_tag": "ABX9TyO1EWjfWQJMvN/IphYmKIgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysfesr/YT-COMMENTS-ANALYSIS/blob/master/Arabic_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBpUBp1xkfjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100793cc-6b88-4487-ad77-cf1863c34b23"
      },
      "source": [
        "import pandas as pd\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rzeO0bBQgtQ6",
        "outputId": "cccb2a90-c8cf-40f7-b8da-fea0eb7eefe3"
      },
      "source": [
        "data = pd.read_csv('drive/MyDrive/colab notebook/Arabe Sentiment Analysis/datasets/arabic_sentiment.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>#Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  label                                               text\n",
              "0           0      0  Ø§Ø¹ØªØ±Ù Ø§Ù† Ø¨ØªØ³ ÙƒØ§Ù†Ùˆ Ø´ÙˆÙŠ Ø´ÙˆÙŠ ÙŠØ¬ÙŠØ¨Ùˆ Ø±Ø§Ø³ÙŠ Ù„ÙƒÙ† Ø§Ù„ÙŠÙˆÙ…...\n",
              "1           1      0  ØªÙˆÙ‚Ø¹Øª Ø§Ø°Ø§ Ø¬Ø§Øª Ø¯Ø§Ø±ÙŠØ§ Ø¨Ø´ÙˆÙÙ‡Ù… ÙƒØ§Ù…Ù„ÙŠÙ† Ø¨Ø³ Ù„ÙŠ Ù„Ù„Ø­ÙŠÙ† ...\n",
              "2           2      0  #Ø§Ù„Ø§Ù‡Ù„ÙŠ_Ø§Ù„Ù‡Ù„Ø§Ù„ Ø§ÙƒØªØ¨ ØªÙˆÙ‚Ø¹Ùƒ Ù„Ù†ØªÙŠØ¬Ø© Ù„Ù‚Ø§Ø¡ Ø§Ù„Ù‡Ù„Ø§Ù„ Ùˆ...\n",
              "3           3      0  Ù†Ø¹Ù…Ø© Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ© . ØªØ¶Ø¹ Ù‚Ø·Ø±Ø©ğŸ’§Ù…Ø¶Ø§Ø¯ Ø¨Ù†Ø³Ù„ÙŠÙ† Ø¹...\n",
              "4           4      0                             Ø§Ù„Ø¯ÙˆØ¯Ùˆ Ø¬Ø§ÙŠÙ‡ ØªÙƒÙ…Ù„ Ø¹Ù„ÙŠ ğŸ’”"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1k2llo9mShG"
      },
      "source": [
        "stop_words = set(stopwords.words('arabic'))\r\n",
        "\r\n",
        "def remove_diacritics(text):\r\n",
        "    arabic_diacritics = re.compile(\"\"\" Ù‘    | # Tashdid\r\n",
        "                             Ù    | # Fatha\r\n",
        "                             Ù‹    | # Tanwin Fath\r\n",
        "                             Ù    | # Damma\r\n",
        "                             ÙŒ    | # Tanwin Damm\r\n",
        "                             Ù    | # Kasra\r\n",
        "                             Ù    | # Tanwin Kasr\r\n",
        "                             Ù’    | # Sukun\r\n",
        "                             Ù€     # Tatwil/Kashida\r\n",
        "                         \"\"\", re.VERBOSE)\r\n",
        "    text = re.sub(arabic_diacritics, '', str(text))\r\n",
        "    return text\r\n",
        "\r\n",
        "def remove_emoji(text):\r\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags = re.UNICODE)\r\n",
        "    return regrex_pattern.sub(r'',text)\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    text = \"\".join([word for word in text if word not in string.punctuation])\r\n",
        "    text = remove_emoji(text)\r\n",
        "    text = remove_diacritics(text)\r\n",
        "    tokens = word_tokenize(text)\r\n",
        "    text = ' '.join([word for word in tokens if word not in stop_words])\r\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "epqSnzrAohuH",
        "outputId": "c51e20ad-8045-45e3-b2c5-0c488f4f5d5e"
      },
      "source": [
        "data = data.drop('Unnamed: 0', axis=1)\r\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\r\n",
        "data.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23431</th>\n",
              "      <td>1</td>\n",
              "      <td>Ø³Ø­Ø¨ Ø¹Ù„Ù‰ Ù…Ø¨Ù„Øº Ù…Ø§Ù„ÙŠ ğŸ’° Ù„Ù…ØªØ§Ø¨Ø¹ÙŠ #ÙƒØ´ÙƒÙˆÙ„ ğŸ‘ğŸ» Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:...</td>\n",
              "      <td>Ø³Ø­Ø¨ Ù…Ø¨Ù„Øº Ù…Ø§Ù„ÙŠ Ù„Ù…ØªØ§Ø¨Ø¹ÙŠ ÙƒØ´ÙƒÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø´ÙŠ Ø±ØªÙˆÙŠØª Ø§...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24029</th>\n",
              "      <td>1</td>\n",
              "      <td>Ø§ÙÙ ÙƒÙŠÙˆØª Ù…Ø±Ø§</td>\n",
              "      <td>Ø§ÙÙ ÙƒÙŠÙˆØª Ù…Ø±Ø§</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35487</th>\n",
              "      <td>1</td>\n",
              "      <td>Ø§Ù‡Ø¨Ù„ Ù…Ø§ÙŠØ¹Ø±ÙÙ†ÙŠ Ø¨Ø§Ù‚ÙŠ ğŸ˜</td>\n",
              "      <td>Ø§Ù‡Ø¨Ù„ Ù…Ø§ÙŠØ¹Ø±ÙÙ†ÙŠ Ø¨Ø§Ù‚ÙŠ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20589</th>\n",
              "      <td>0</td>\n",
              "      <td>Ø§ØµØ·Ø¨Ø­ÙŠ ÙˆÙ‚ÙˆÙ„ÙŠ ÙŠØ§ ØµØ¨Ø­ ğŸ˜</td>\n",
              "      <td>Ø§ØµØ·Ø¨Ø­ÙŠ ÙˆÙ‚ÙˆÙ„ÙŠ ØµØ¨Ø­</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26301</th>\n",
              "      <td>1</td>\n",
              "      <td>Ù„Ø§Ø¹Ø¨ #Ø§Ù„Ù‡Ù„Ø§Ù„ ÙƒÙ†Ùˆ ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ø§Ù…Ø³: Ø§Ù„ØªÙ…Ø±ÙŠØ±Ø§Øª âœ… ...</td>\n",
              "      <td>Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù‡Ù„Ø§Ù„ ÙƒÙ†Ùˆ Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ø§Ù…Ø³ Ø§Ù„ØªÙ…Ø±ÙŠØ±Ø§Øª âœ… Ø¯Ù‚Ø© Ø§...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ...                                       cleaned_text\n",
              "23431      1  ...  Ø³Ø­Ø¨ Ù…Ø¨Ù„Øº Ù…Ø§Ù„ÙŠ Ù„Ù…ØªØ§Ø¨Ø¹ÙŠ ÙƒØ´ÙƒÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø´ÙŠ Ø±ØªÙˆÙŠØª Ø§...\n",
              "24029      1  ...                                       Ø§ÙÙ ÙƒÙŠÙˆØª Ù…Ø±Ø§\n",
              "35487      1  ...                                 Ø§Ù‡Ø¨Ù„ Ù…Ø§ÙŠØ¹Ø±ÙÙ†ÙŠ Ø¨Ø§Ù‚ÙŠ\n",
              "20589      0  ...                                   Ø§ØµØ·Ø¨Ø­ÙŠ ÙˆÙ‚ÙˆÙ„ÙŠ ØµØ¨Ø­\n",
              "26301      1  ...  Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù‡Ù„Ø§Ù„ ÙƒÙ†Ùˆ Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ø§Ù…Ø³ Ø§Ù„ØªÙ…Ø±ÙŠØ±Ø§Øª âœ… Ø¯Ù‚Ø© Ø§...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB33APlFojf5"
      },
      "source": [
        "# split the data to tain and test data\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'], test_size=0.3, stratify=data['label'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G1vzFw4qFn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b33f74-143d-4bd4-a6cb-e82211d2c262"
      },
      "source": [
        "steps = [('vectorizer', TfidfVectorizer(min_df=0.0001, max_df=0.95, analyzer='word', lowercase=False)), ('cls', RandomForestClassifier())]\r\n",
        "pipeline = Pipeline(steps) # define the pipeline object.\r\n",
        "parameteres = {'vectorizer__max_features':[1000,3000,7000,10000]}\r\n",
        "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\r\n",
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vectorizer',\n",
              "                                        TfidfVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.float64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=False,\n",
              "                                                        max_df=0.95,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=0.0001,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        norm='l2',\n",
              "                                                        preprocessor=None,\n",
              "                                                        smooth_idf=True,\n",
              "                                                        stop_words=No...\n",
              "                                                               min_samples_leaf=1,\n",
              "                                                               min_samples_split=2,\n",
              "                                                               min_weight_fraction_leaf=0.0,\n",
              "                                                               n_estimators=100,\n",
              "                                                               n_jobs=None,\n",
              "                                                               oob_score=False,\n",
              "                                                               random_state=None,\n",
              "                                                               verbose=0,\n",
              "                                                               warm_start=False))],\n",
              "                                verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'vectorizer__max_features': [1000, 3000, 7000, 10000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bQdUd3uqxug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0859c4fa-1357-45b7-8592-99e3a5da4325"
      },
      "source": [
        "print(\"best estimator: \",grid.best_estimator_)\r\n",
        "print(\"best score: \", grid.best_score_)\r\n",
        "print(\"best params: \", grid.best_params_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best estimator:  Pipeline(memory=None,\n",
            "         steps=[('vectorizer',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.float64'>,\n",
            "                                 encoding='utf-8', input='content',\n",
            "                                 lowercase=False, max_df=0.95,\n",
            "                                 max_features=10000, min_df=0.0001,\n",
            "                                 ngram_range=(1, 1), norm='l2',\n",
            "                                 preprocessor=None, smooth_idf=True,\n",
            "                                 stop_words=None, strip_accents=None,\n",
            "                                 sublinear_tf=False,\n",
            "                                 to...\n",
            "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
            "                                        class_weight=None, criterion='gini',\n",
            "                                        max_depth=None, max_features='auto',\n",
            "                                        max_leaf_nodes=None, max_samples=None,\n",
            "                                        min_impurity_decrease=0.0,\n",
            "                                        min_impurity_split=None,\n",
            "                                        min_samples_leaf=1, min_samples_split=2,\n",
            "                                        min_weight_fraction_leaf=0.0,\n",
            "                                        n_estimators=100, n_jobs=None,\n",
            "                                        oob_score=False, random_state=None,\n",
            "                                        verbose=0, warm_start=False))],\n",
            "         verbose=False)\n",
            "best score:  0.7754553038264804\n",
            "best params:  {'vectorizer__max_features': 10000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-N9ZBLUq36n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6237867-f85f-4aaf-a01e-89a0a339d87a"
      },
      "source": [
        "y_pred = grid.predict(X_test)\r\n",
        "precision = precision_score(y_test, y_pred)\r\n",
        "recall = recall_score(y_test, y_pred)\r\n",
        "accuracy = accuracy_score(y_test, y_pred)\r\n",
        "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round(accuracy, 3)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.799 / Recall: 0.749 / Accuracy: 0.779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGhzP_0qAMUK",
        "outputId": "66984add-b7c1-45d3-c94c-23aea2869984"
      },
      "source": [
        "from sklearn.externals import joblib\r\n",
        "\r\n",
        "model_save_name = 'sentiment_classifier.pkl'\r\n",
        "path = F\"drive/MyDrive/colab notebook/Arabe Sentiment Analysis/{model_save_name}\"\r\n",
        "\r\n",
        "joblib.dump(grid.best_estimator_, path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/MyDrive/colab notebook/Arabe Sentiment Analysis/sentiment_classifier.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}